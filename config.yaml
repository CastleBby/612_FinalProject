# ============================================================================
# Configuration for Enhanced Precipitation Forecasting Project
# With Geographic Attention and Multi-Scale Temporal Layers
# ============================================================================

# REPRODUCIBILITY SETTINGS
reproducibility:
  random_seed: 202511  # Fixed seed for reproducible results
  deterministic: true   # Use deterministic algorithms

data:
  start_date: '2000-01-01'
  end_date: '2024-12-31'  # Extended to 25 years for better generalization
  locations:  # Lat/Lon for 5 representative MD stations (cities as proxies)
    - name: 'Baltimore'
      lat: 39.29
      lon: -76.61
    - name: 'Annapolis'
      lat: 38.98
      lon: -76.49
    - name: 'Cumberland'
      lat: 39.62
      lon: -78.76
    - name: 'Ocean City'
      lat: 38.34
      lon: -75.08
    - name: 'Hagerstown'
      lat: 39.64
      lon: -77.72
  variables: ['temperature_2m', 'relative_humidity_2m', 'precipitation', 'pressure_msl', 'wind_speed_10m']
  seq_len: 24  # Input sequence length (hours)
  pred_horizon: 1  # Predict next 1 hour
  test_split: 0.15  # For eval; adjust for full data
  val_split: 0.15

model:
  # Architecture parameters (V4: Enhanced Classification)
  # ⚠️ REDUCED CAPACITY: 512→256 to prevent overfitting (20.8M→5.2M params)
  # With only 767K training samples and 5 locations, 20.8M params was excessive
  d_model: 256  # Model capacity (REDUCED by 50%)
  nhead: 8
  num_encoder_layers: 4  # Encoder depth (bi-directional)
  num_decoder_layers: 2  # Decoder depth (with cross-attention)
  num_layers: 6  # For backwards compatibility with old models
  embed_dim: 128  # Embedding dimension
  dropout: 0.3  # INCREASED dropout to prevent overfitting (was 0.2)
  
  # Advanced layer controls (V4: Enhanced architecture)
  use_advanced_layers: true  # Enable: multi-scale attention, enhanced classification head
  use_physics_loss: false     # Disable physics constraints (too restrictive)
  physics_weight: 0.05       # Light physics weight if enabled
  
  # Training parameters (V4: Optimized for classification)
  lr: 0.00008  # Slightly lower LR for stable focal loss training
  batch_size: 128  # Larger batch for stable gradients
  epochs: 120  # More epochs for focal loss convergence
  device: 'mps'  # For M1 Max acceleration (options: 'cuda', 'mps', 'cpu')
  gradient_clip: 1.0  # Gradient clipping for stability (prevent exploding gradients)
  weight_decay: 0.015  # Slightly higher L2 regularization
  warmup_epochs: 10  # Learning rate warmup
  patience: 20  # Increased patience for focal loss convergence
  
  # Feature grouping for domain-aware embeddings (total: 11 features = 5 weather + 6 temporal)
  feature_groups:
    thermo: [0, 1]          # temperature_2m, relative_humidity_2m
    hydro: [2, 3]           # precipitation, pressure_msl
    dynamic: [4]            # wind_speed_10m
    temporal_diurnal: [5, 6]  # hour_sin, hour_cos (daily cycle)
    temporal_seasonal: [7, 8, 9, 10]  # day_sin, day_cos, month_sin, month_cos (seasonal patterns)
  
  # Loss function parameters
  extreme_weight: 10.0  # Increased weight for heavy rain events

eval:
  extreme_threshold: 0.9  # 90th percentile for extremes
  rain_threshold: 0.1  # mm for event detection
  prediction_threshold: 0.20  # mm - predictions below this are set to 0 (OPTIMIZED on 2000-2024 data: maximizes CSI=0.5739)

# V5 IMPROVED: Anti-Overfitting Enhanced Transformer (Transformer-based)
v5:
  output_dir: 'v5_results'
  
  # VQ-VAE baseline integration (RECOMMENDED for noise reduction)
  vqvae_checkpoint: 'nowcasting_baseline_results/vqvae_best.pth'  # Pre-trained VQ-VAE
  use_vqvae: true  # ENABLED: VQ-VAE features help reduce noise
  freeze_vqvae: true  # Freeze VQ-VAE weights
  
  # Transformer architecture (OPTIMIZED: Deeper but better regularized)
  d_model: 768  # Model dimension (increased significantly for better capacity)
  nhead: 12  # Number of attention heads (increased for more capacity)
  num_encoder_layers: 8  # Transformer layers (deeper - 6→8)
  dim_feedforward: 3072  # FFN dimension (4x d_model, increased)
  dropout: 0.2  # Transformer dropout (balanced - not too aggressive)
  head_dropout: 0.3  # Dropout for output heads (balanced)
  stochastic_depth: 0.05  # Layer dropout (reduced - less aggressive)
  use_probsparse: true  # Use ProbSparse attention (Informer)
  
  # Training parameters (ADJUSTED for larger model)
  batch_size: 128  # Reduced for larger model (58M params)
  gradient_accumulation_steps: 4  # Effective batch size: 512 (maintained)
  lr: 0.0001  # Learning rate (stable)
  weight_decay: 0.0005  # L2 regularization (balanced - not too aggressive)
  epochs: 200  # Max epochs (increased for deeper training)
  gradient_clip: 1.0  # Gradient clipping
  patience: 30  # Early stopping patience (increased to allow deeper training)
  warmup_steps: 2000  # LR warmup steps (increased for deeper model)
  
  # Multi-task loss weights (HEAVILY FAVOR CLASSIFICATION TO FIX FN)
  regression_weight: 0.3  # Reduced (regression less important)
  classification_weight: 0.7  # Increased (classification critical - fix false negatives)
  
  # Focal loss parameters (HEAVILY PENALIZE FALSE NEGATIVES)
  focal_alpha: 0.8  # Weight for positive class (rain) - HEAVILY favor rain detection
  focal_gamma: 5.0   # Focusing parameter (increased - focus hard on missed rain)
  label_smoothing: 0.05  # Label smoothing (reduced - need sharper predictions)
  
  # Data augmentation (NEW for anti-overfitting)
  mixup_alpha: 0.2  # Mixup augmentation parameter
  noise_std: 0.01  # Gaussian noise std for input
  
  # Extreme event handling (NEW)
  extreme_weight: 2.0  # Weight multiplier for extreme events
  extreme_threshold_percentile: 0.9  # 90th percentile
